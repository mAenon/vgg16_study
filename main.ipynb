{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cofing: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "batch_size = 100 # データの大きさ reshapeしてからshapeの形求める感じでやろう\n",
    "timesteps = 256\n",
    "input_dim = 3 # acc (x, y, z)\n",
    "num_classes = 7 # 分類数\n",
    "\n",
    "x_train, y_train, x_test, y_test = (1, 1, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "label_path = home_dir + \"/Desktop/hasc_data/y_large3.csv\"\n",
    "data_path  = home_dir + \"/Desktop/hasc_data/x_large3.csv\"\n",
    "\n",
    "label_df = pd.read_csv(label_path, index_col=0)\n",
    "data_np  = np.loadtxt (data_path , delimiter=\",\").reshape(-1, timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n36866    5\n36867    5\n36868    5\n36869    5\n36870    5\nName: act, Length: 36871, dtype: int64\n"
    }
   ],
   "source": [
    "# label の取り扱い\n",
    "# 0 to 5 :  6 class\n",
    "labels = label_df[\"act\"] - 1\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データ分割\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(data_np, labels, test_size = 0.2, train_size=None, stratify=labels)\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "num_classes = len(labels.unique())\n",
    "batch_size = x_train.shape[0]\n",
    "print(num_classes, \"classies validation\")\n",
    "print(\"train data len :\", batch_size)\n",
    "print(\"test  data len :\", x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "# TODO\n",
    "# ここもっと綺麗にしたい\n",
    "# dict使って，MMSとStdSをうまく使えるようにしたい\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "MMScaler = MinMaxScaler()\n",
    "StdScaler = StandardScaler()\n",
    "\n",
    "datas = {\"original\" : [x_train, x_train]}\n",
    "x_train_mms  = []\n",
    "x_test_mms   = []\n",
    "x_train_stds = []\n",
    "x_test_stds  = []\n",
    "\n",
    "for data in x_train:\n",
    "    x = MMScaler.fit_transform(data)\n",
    "    x_train_mms.append(x)\n",
    "    x = StdScaler.fit_transform(data)\n",
    "    x_train_stds.append(x)\n",
    "\n",
    "for data in x_test:\n",
    "    x = MMScaler.fit_transform(data)\n",
    "    x_test_mms.append(x)\n",
    "    x = StdScaler.fit_transform(data)\n",
    "    x_test_stds.append(x)\n",
    "\n",
    "x_train_mms  = np.array(x_train_mms )\n",
    "x_test_mms   = np.array(x_test_mms  )\n",
    "x_train_stds = np.array(x_train_stds)\n",
    "x_test_stds  = np.array(x_test_stds )\n",
    "\n",
    "datas[\"mms\"] = [x_train_mms,  x_test_mms]\n",
    "datas[\"stds\"]= [x_train_stds, x_test_stds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO モデルを保存したdictを作ってうまく管理したい\n",
    "# 今はモデル作ったやつをそのまま辞書の中に入れてるけど，コンパイルしてから入れたほうがいいかも？\n",
    "\n",
    "models = {}\n",
    "\n",
    "lstm_dontrun_input = Input( shape=( timesteps, input_dim ) )\n",
    "lstm_dontrun_lstm1 = LSTM(units=256, activation=\"relu\", kernel_initializer=\"he_normal\", return_sequences=True, dropout=0.1)( lstm_dontrun_input )\n",
    "lstm_dontrun_lstm2 = LSTM(units=128, activation=\"relu\", kernel_initializer=\"he_normal\", return_sequences=True, dropout=0.1)( lstm_dontrun_lstm1 )\n",
    "lstm_dontrun_lstm3 = LSTM(units=64, activation=\"relu\", kernel_initializer=\"he_normal\",  return_sequences=True, dropout=0.1)( lstm_dontrun_lstm2 )\n",
    "lstm_dontrun_lstm4 = LSTM(units=32, activation=\"relu\", kernel_initializer=\"he_normal\", dropout=0.1)( lstm_dontrun_lstm3 )\n",
    "lstm_dontrun_dense = Dense(units=num_classes, activation=\"softmax\")( lstm_dontrun_lstm4 )\n",
    "model_lstm_dontrun = Model(inputs = lstm_dontrun_input, outputs=lstm_dontrun_dense, name=\"lstm_dontrun\")\n",
    "models[\"lstm_dontrun\"] = model_lstm_dontrun\n",
    "\n",
    "lstm_1_input = Input( shape=( timesteps, input_dim ) )\n",
    "lstm_1_lstm = LSTM(units=32, activation=\"relu\", kernel_initializer=\"he_normal\",  return_sequences=False, dropout=0.1)( lstm_1_input )\n",
    "lstm_1_dense = Dense(units=num_classes, activation=\"softmax\")( lstm_1_lstm )\n",
    "model_lstm_1 = Model(inputs = lstm_1_input, outputs = lstm_1_dense, name=\"lstm_1\")\n",
    "models[\"lstm_1\"] = model_lstm_1\n",
    "\n",
    "conv1_1_input = Input( shape=( timesteps, input_dim ) )\n",
    "conv1_1_conv1 = Conv1D(filters=128, kernel_size=16, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_1_input)\n",
    "conv1_1_pool1 = MaxPooling1D(4, padding='same')(conv1_1_conv1)\n",
    "conv1_1_conv2 = Conv1D(filters=64, kernel_size=16, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_1_pool1)\n",
    "conv1_1_pool2 = MaxPooling1D(1, padding='same')(conv1_1_conv2)\n",
    "conv1_1_conv3 = Conv1D(filters=32, kernel_size=16, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_1_pool2)\n",
    "conv1_1_conv4 = Conv1D(filters=16, kernel_size=16, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_1_conv3)\n",
    "conv1_1_flatter = Flatten()(conv1_1_conv4)\n",
    "conv1_1_dense = Dense(units=num_classes, activation=\"softmax\")(conv1_1_flatter)\n",
    "model_conv1_1 = Model(inputs=conv1_1_input, outputs=conv1_1_dense, name=\"conv1d_1\")\n",
    "models[\"conv1d_1\"] = model_conv1_1\n",
    "\n",
    "\n",
    "\n",
    "conv1_2_input = Input( shape=( timesteps, input_dim ) )\n",
    "conv1_2_conv1 = Conv1D(filters=64, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_2_input)\n",
    "conv1_2_dropout1 = Dropout(0.25)(conv1_2_conv1)\n",
    "# conv1_2_pool1 = MaxPooling1D(4, padding='same')(conv1_1_conv1)\n",
    "conv1_2_conv2 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_2_dropout1)\n",
    "conv1_2_dropout2 = Dropout(0.25)(conv1_2_conv2)\n",
    "# conv1_2_pool2 = MaxPooling1D(1, padding='same')(conv1_1_conv2)\n",
    "conv1_2_conv3 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_2_dropout2)\n",
    "conv1_2_dropout3 = Dropout(0.25)(conv1_2_conv3)\n",
    "conv1_2_conv4 = Conv1D(filters=256, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\", activation=\"relu\")(conv1_2_dropout3)\n",
    "conv1_2_dropout4 = Dropout(0.25)(conv1_2_conv4)\n",
    "conv1_2_flatter = Flatten()(conv1_2_dropout4)\n",
    "conv1_2_dense1 = Dense(units=2048, activation=\"relu\")(conv1_2_flatter)\n",
    "conv1_2_dense2 = Dense(units=num_classes, activation=\"softmax\")(conv1_2_dense1)\n",
    "model_conv1_2 = Model(inputs=conv1_2_input, outputs=conv1_2_dense2, name=\"conv1d_2\")\n",
    "models[\"conv1d_2\"] = model_conv1_2\n",
    "\n",
    "\n",
    "# 1d vgg16\n",
    "vgg16_input = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "# block1\n",
    "vgg16_block1_conv1 = Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_input)\n",
    "vgg16_block1_bn1 = BatchNormalization()(vgg16_block1_conv1)\n",
    "vgg16_block1_actiovation1 = Activation(\"relu\")(vgg16_block1_bn1)\n",
    "vgg16_block1_conv2 = Conv1D(filters=64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block1_actiovation1)\n",
    "vgg16_block1_bn2 = BatchNormalization()(vgg16_block1_conv2)\n",
    "vgg16_block1_actiovation2 = Activation(\"relu\")(vgg16_block1_bn2)\n",
    "vgg16_block1_pool = MaxPooling1D(pool_size=2, strides=2, padding=\"same\")(vgg16_block1_actiovation2)\n",
    "\n",
    "# block2\n",
    "vgg16_block2_conv1 = Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block1_pool)\n",
    "vgg16_block2_bn1 = BatchNormalization()(vgg16_block2_conv1)\n",
    "vgg16_block2_actiovation1 = Activation(\"relu\")(vgg16_block2_bn1)\n",
    "vgg16_block2_conv2 = Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block2_actiovation1)\n",
    "vgg16_block2_bn2 = BatchNormalization()(vgg16_block2_conv2)\n",
    "vgg16_block2_actiovation2 = Activation(\"relu\")(vgg16_block2_bn2)\n",
    "vgg16_block2_pool = MaxPooling1D(pool_size=2, strides=2, padding=\"same\")(vgg16_block2_actiovation2)\n",
    "\n",
    "# block3\n",
    "vgg16_block3_conv1 = Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block2_pool)\n",
    "vgg16_block3_bn1 = BatchNormalization()(vgg16_block3_conv1)\n",
    "vgg16_block3_actiovation1 = Activation(\"relu\")(vgg16_block3_bn1)\n",
    "vgg16_block3_conv2 = Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block3_actiovation1)\n",
    "vgg16_block3_bn2 = BatchNormalization()(vgg16_block3_conv2)\n",
    "vgg16_block3_actiovation2 = Activation(\"relu\")(vgg16_block3_bn2)\n",
    "vgg16_block3_conv3 = Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block3_actiovation2)\n",
    "vgg16_block3_bn3 = BatchNormalization()(vgg16_block3_conv3)\n",
    "vgg16_block3_actiovation3 = Activation(\"relu\")(vgg16_block3_bn3)\n",
    "vgg16_block3_pool = MaxPooling1D(pool_size=2, strides=2, padding=\"same\")(vgg16_block3_actiovation3)\n",
    "\n",
    "# block4\n",
    "vgg16_block4_conv1 = Conv1D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block3_pool)\n",
    "vgg16_block4_bn1 = BatchNormalization()(vgg16_block4_conv1)\n",
    "vgg16_block4_actiovation1 = Activation(\"relu\")(vgg16_block4_bn1)\n",
    "vgg16_block4_conv2 = Conv1D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block4_actiovation1)\n",
    "vgg16_block4_bn2 = BatchNormalization()(vgg16_block4_conv2)\n",
    "vgg16_block4_actiovation2 = Activation(\"relu\")(vgg16_block4_bn2)\n",
    "vgg16_block4_conv3 = Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block4_actiovation2)\n",
    "vgg16_block4_bn3 = BatchNormalization()(vgg16_block4_conv3)\n",
    "vgg16_block4_actiovation3 = Activation(\"relu\")(vgg16_block4_bn3)\n",
    "vgg16_block4_pool = MaxPooling1D(pool_size=2, strides=2, padding=\"same\")(vgg16_block4_actiovation3)\n",
    "\n",
    "# block5\n",
    "vgg16_block5_conv1 = Conv1D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block4_pool)\n",
    "vgg16_block5_bn1 = BatchNormalization()(vgg16_block5_conv1)\n",
    "vgg16_block5_actiovation1 = Activation(\"relu\")(vgg16_block5_bn1)\n",
    "vgg16_block5_conv2 = Conv1D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block5_actiovation1)\n",
    "vgg16_block5_bn2 = BatchNormalization()(vgg16_block5_conv2)\n",
    "vgg16_block5_actiovation2 = Activation(\"relu\")(vgg16_block5_bn2)\n",
    "vgg16_block5_conv3 = Conv1D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(vgg16_block5_actiovation2)\n",
    "vgg16_block5_bn3 = BatchNormalization()(vgg16_block5_conv3)\n",
    "vgg16_block5_actiovation3 = Activation(\"relu\")(vgg16_block5_bn3)\n",
    "vgg16_block5_pool = MaxPooling1D(pool_size=2, strides=2, padding=\"same\")(vgg16_block5_actiovation3)\n",
    "\n",
    "# dense\n",
    "vgg16_flatten = Flatten()(vgg16_block5_pool)\n",
    "vgg16_dense1  = Dense(units=4096, activation=\"relu\")(vgg16_flatten)\n",
    "vgg16_dense2  = Dense(units=4096, activation=\"relu\")(vgg16_dense1)\n",
    "vgg16_output  = Dense(units=num_classes, activation=\"softmax\")(vgg16_dense2)\n",
    "model_vgg16_1d = Model(inputs = vgg16_input, outputs=vgg16_output, name=\"vgg16_1d\")\n",
    "models[\"vgg16_1d\"] = model_vgg16_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model list:\n・ lstm_dontrun\n・ lstm_1\n・ conv1d_1\n・ conv1d_2\n・ vgg16_1d\n\nModel: \"lstm_dontrun\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_46 (InputLayer)        [(None, 256, 3)]          0         \n_________________________________________________________________\nlstm_45 (LSTM)               (None, 256, 256)          266240    \n_________________________________________________________________\nlstm_46 (LSTM)               (None, 256, 128)          197120    \n_________________________________________________________________\nlstm_47 (LSTM)               (None, 256, 64)           49408     \n_________________________________________________________________\nlstm_48 (LSTM)               (None, 32)                12416     \n_________________________________________________________________\ndense_62 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 525,382\nTrainable params: 525,382\nNon-trainable params: 0\n_________________________________________________________________\n\n\nModel: \"lstm_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_47 (InputLayer)        [(None, 256, 3)]          0         \n_________________________________________________________________\nlstm_49 (LSTM)               (None, 32)                4608      \n_________________________________________________________________\ndense_63 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 4,806\nTrainable params: 4,806\nNon-trainable params: 0\n_________________________________________________________________\n\n\nModel: \"conv1d_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_48 (InputLayer)        [(None, 256, 3)]          0         \n_________________________________________________________________\nconv1d_154 (Conv1D)          (None, 256, 128)          6272      \n_________________________________________________________________\nmax_pooling1d_49 (MaxPooling (None, 64, 128)           0         \n_________________________________________________________________\nconv1d_155 (Conv1D)          (None, 64, 64)            131136    \n_________________________________________________________________\nmax_pooling1d_50 (MaxPooling (None, 64, 64)            0         \n_________________________________________________________________\nconv1d_156 (Conv1D)          (None, 64, 32)            32800     \n_________________________________________________________________\nconv1d_157 (Conv1D)          (None, 64, 16)            8208      \n_________________________________________________________________\nflatten_24 (Flatten)         (None, 1024)              0         \n_________________________________________________________________\ndense_64 (Dense)             (None, 6)                 6150      \n=================================================================\nTotal params: 184,566\nTrainable params: 184,566\nNon-trainable params: 0\n_________________________________________________________________\n\n\nModel: \"conv1d_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_49 (InputLayer)        [(None, 256, 3)]          0         \n_________________________________________________________________\nconv1d_158 (Conv1D)          (None, 256, 64)           640       \n_________________________________________________________________\ndropout_36 (Dropout)         (None, 256, 64)           0         \n_________________________________________________________________\nconv1d_159 (Conv1D)          (None, 256, 128)          24704     \n_________________________________________________________________\ndropout_37 (Dropout)         (None, 256, 128)          0         \n_________________________________________________________________\nconv1d_160 (Conv1D)          (None, 256, 128)          49280     \n_________________________________________________________________\ndropout_38 (Dropout)         (None, 256, 128)          0         \n_________________________________________________________________\nconv1d_161 (Conv1D)          (None, 256, 256)          98560     \n_________________________________________________________________\ndropout_39 (Dropout)         (None, 256, 256)          0         \n_________________________________________________________________\nflatten_25 (Flatten)         (None, 65536)             0         \n_________________________________________________________________\ndense_65 (Dense)             (None, 2048)              134219776 \n_________________________________________________________________\ndense_66 (Dense)             (None, 6)                 12294     \n=================================================================\nTotal params: 134,405,254\nTrainable params: 134,405,254\nNon-trainable params: 0\n_________________________________________________________________\n\n\nModel: \"vgg16_1d\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_50 (InputLayer)        [(None, 256, 3)]          0         \n_________________________________________________________________\nconv1d_162 (Conv1D)          (None, 256, 64)           640       \n_________________________________________________________________\nbatch_normalization_80 (Batc (None, 256, 64)           256       \n_________________________________________________________________\nactivation_80 (Activation)   (None, 256, 64)           0         \n_________________________________________________________________\nconv1d_163 (Conv1D)          (None, 256, 64)           12352     \n_________________________________________________________________\nbatch_normalization_81 (Batc (None, 256, 64)           256       \n_________________________________________________________________\nactivation_81 (Activation)   (None, 256, 64)           0         \n_________________________________________________________________\nmax_pooling1d_51 (MaxPooling (None, 128, 64)           0         \n_________________________________________________________________\nconv1d_164 (Conv1D)          (None, 128, 128)          24704     \n_________________________________________________________________\nbatch_normalization_82 (Batc (None, 128, 128)          512       \n_________________________________________________________________\nactivation_82 (Activation)   (None, 128, 128)          0         \n_________________________________________________________________\nconv1d_165 (Conv1D)          (None, 128, 128)          49280     \n_________________________________________________________________\nbatch_normalization_83 (Batc (None, 128, 128)          512       \n_________________________________________________________________\nactivation_83 (Activation)   (None, 128, 128)          0         \n_________________________________________________________________\nmax_pooling1d_52 (MaxPooling (None, 64, 128)           0         \n_________________________________________________________________\nconv1d_166 (Conv1D)          (None, 64, 256)           98560     \n_________________________________________________________________\nbatch_normalization_84 (Batc (None, 64, 256)           1024      \n_________________________________________________________________\nactivation_84 (Activation)   (None, 64, 256)           0         \n_________________________________________________________________\nconv1d_167 (Conv1D)          (None, 64, 256)           196864    \n_________________________________________________________________\nbatch_normalization_85 (Batc (None, 64, 256)           1024      \n_________________________________________________________________\nactivation_85 (Activation)   (None, 64, 256)           0         \n_________________________________________________________________\nconv1d_168 (Conv1D)          (None, 64, 256)           196864    \n_________________________________________________________________\nbatch_normalization_86 (Batc (None, 64, 256)           1024      \n_________________________________________________________________\nactivation_86 (Activation)   (None, 64, 256)           0         \n_________________________________________________________________\nmax_pooling1d_53 (MaxPooling (None, 32, 256)           0         \n_________________________________________________________________\nconv1d_169 (Conv1D)          (None, 32, 512)           393728    \n_________________________________________________________________\nbatch_normalization_87 (Batc (None, 32, 512)           2048      \n_________________________________________________________________\nactivation_87 (Activation)   (None, 32, 512)           0         \n_________________________________________________________________\nconv1d_170 (Conv1D)          (None, 32, 512)           786944    \n_________________________________________________________________\nbatch_normalization_88 (Batc (None, 32, 512)           2048      \n_________________________________________________________________\nactivation_88 (Activation)   (None, 32, 512)           0         \n_________________________________________________________________\nconv1d_171 (Conv1D)          (None, 32, 256)           393472    \n_________________________________________________________________\nbatch_normalization_89 (Batc (None, 32, 256)           1024      \n_________________________________________________________________\nactivation_89 (Activation)   (None, 32, 256)           0         \n_________________________________________________________________\nmax_pooling1d_54 (MaxPooling (None, 16, 256)           0         \n_________________________________________________________________\nconv1d_172 (Conv1D)          (None, 16, 512)           393728    \n_________________________________________________________________\nbatch_normalization_90 (Batc (None, 16, 512)           2048      \n_________________________________________________________________\nactivation_90 (Activation)   (None, 16, 512)           0         \n_________________________________________________________________\nconv1d_173 (Conv1D)          (None, 16, 512)           786944    \n_________________________________________________________________\nbatch_normalization_91 (Batc (None, 16, 512)           2048      \n_________________________________________________________________\nactivation_91 (Activation)   (None, 16, 512)           0         \n_________________________________________________________________\nconv1d_174 (Conv1D)          (None, 16, 256)           393472    \n_________________________________________________________________\nbatch_normalization_92 (Batc (None, 16, 256)           1024      \n_________________________________________________________________\nactivation_92 (Activation)   (None, 16, 256)           0         \n_________________________________________________________________\nmax_pooling1d_55 (MaxPooling (None, 8, 256)            0         \n_________________________________________________________________\nflatten_26 (Flatten)         (None, 2048)              0         \n_________________________________________________________________\ndense_67 (Dense)             (None, 4096)              8392704   \n_________________________________________________________________\ndense_68 (Dense)             (None, 4096)              16781312  \n_________________________________________________________________\ndense_69 (Dense)             (None, 6)                 24582     \n=================================================================\nTotal params: 28,940,998\nTrainable params: 28,933,574\nNon-trainable params: 7,424\n_________________________________________________________________\n\n"
    }
   ],
   "source": [
    "print(\"model list:\")\n",
    "for name in models.keys():\n",
    "    print(\"・\", name)\n",
    "\n",
    "for model in models.values():\n",
    "    print()\n",
    "    model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_to_dotがうまく動かないのでこのセルは実行しないこと\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog=\"dot\", format=\"svg\"))\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 29496 samples, validate on 7375 samples\nEpoch 1/50\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-e7ab6631f12d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model_lstm.fit(x_train_mms, y_train, batch_size=batch_size//20, epochs=50, verbose=1, callbacks=[EarlyStopping(verbose=1)], validation_data=(x_test_mms, y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# result = models[\"vgg16_1d\"].fit(x_train_mms, y_train, batch_size=batch_size//20, epochs=50, verbose=1, validation_data=(x_test_mms, y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vgg16_1d\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"stds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model_lstm.fit(x_train_mms, y_train, batch_size=batch_size//20, epochs=50, verbose=1, callbacks=[EarlyStopping(verbose=1)], validation_data=(x_test_mms, y_test))\n",
    "# result = models[\"vgg16_1d\"].fit(x_train_mms, y_train, batch_size=batch_size//20, epochs=50, verbose=1, validation_data=(x_test_mms, y_test))\n",
    "result = models[\"vgg16_1d\"].fit(datas[\"stds\"][0], y_train, batch_size=batch_size//20, epochs=50, verbose=1, validation_data=(datas[\"stds\"][1], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 362.5625 248.518125\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 362.5625 248.518125 \r\nL 362.5625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\nL 355.3625 7.2 \r\nL 20.5625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma4ccdf67e2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(32.599432 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.89571\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(91.53321 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.010737\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(153.648237 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"222.125765\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(215.763265 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.240793\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(277.878293 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.355821\" xlink:href=\"#ma4ccdf67e2\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(339.993321 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m964c5165bd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m964c5165bd\" y=\"222.563639\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(7.2 226.362858)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m964c5165bd\" y=\"174.744075\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(7.2 178.543294)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m964c5165bd\" y=\"126.924511\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(7.2 130.72373)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m964c5165bd\" y=\"79.104947\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(7.2 82.904165)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m964c5165bd\" y=\"31.285382\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(7.2 35.084601)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p36f9ffa383)\" d=\"M 35.780682 90.886326 \r\nL 41.992185 134.869041 \r\nL 48.203687 136.507169 \r\nL 54.41519 136.733006 \r\nL 60.626693 137.012397 \r\nL 66.838196 137.083 \r\nL 73.049699 139.427463 \r\nL 79.261201 139.220924 \r\nL 85.472704 142.490364 \r\nL 91.684207 141.948737 \r\nL 97.89571 143.424911 \r\nL 104.107212 143.032301 \r\nL 110.318715 144.874611 \r\nL 116.530218 142.01209 \r\nL 122.741721 144.280091 \r\nL 128.953224 142.247583 \r\nL 135.164726 144.732486 \r\nL 141.376229 145.032335 \r\nL 147.587732 142.832255 \r\nL 153.799235 143.615508 \r\nL 160.010737 143.599978 \r\nL 166.22224 138.952409 \r\nL 172.433743 143.061908 \r\nL 178.645246 146.5633 \r\nL 184.856749 147.262287 \r\nL 191.068251 147.612017 \r\nL 197.279754 148.068984 \r\nL 203.491257 151.072974 \r\nL 209.70276 148.219835 \r\nL 215.914263 147.506789 \r\nL 222.125765 149.506689 \r\nL 228.337268 147.853703 \r\nL 234.548771 149.402823 \r\nL 240.760274 150.362954 \r\nL 246.971776 150.731311 \r\nL 253.183279 151.680297 \r\nL 259.394782 148.241361 \r\nL 265.606285 150.66921 \r\nL 271.817788 151.529453 \r\nL 278.02929 149.50948 \r\nL 284.240793 148.233965 \r\nL 290.452296 149.141411 \r\nL 296.663799 151.603622 \r\nL 302.875301 144.791949 \r\nL 309.086804 150.698667 \r\nL 315.298307 153.539478 \r\nL 321.50981 154.626121 \r\nL 327.721313 155.837958 \r\nL 333.932815 155.295256 \r\nL 340.144318 156.924776 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p36f9ffa383)\" d=\"M 35.780682 213.853647 \r\nL 41.992185 213.170511 \r\nL 48.203687 214.195216 \r\nL 54.41519 214.024431 \r\nL 60.626693 214.622176 \r\nL 66.838196 212.999726 \r\nL 73.049699 209.669435 \r\nL 79.261201 211.206493 \r\nL 85.472704 209.584043 \r\nL 91.684207 208.730122 \r\nL 97.89571 209.071691 \r\nL 104.107212 210.352572 \r\nL 110.318715 208.644731 \r\nL 116.530218 208.388555 \r\nL 122.741721 209.327868 \r\nL 128.953224 210.437964 \r\nL 135.164726 210.181788 \r\nL 141.376229 209.754828 \r\nL 147.587732 209.327868 \r\nL 153.799235 210.523356 \r\nL 160.010737 210.267179 \r\nL 166.22224 212.828942 \r\nL 172.433743 209.669435 \r\nL 178.645246 208.217769 \r\nL 184.856749 208.388555 \r\nL 191.068251 209.071691 \r\nL 197.279754 209.327868 \r\nL 203.491257 207.534633 \r\nL 209.70276 208.132378 \r\nL 215.914263 207.705418 \r\nL 222.125765 208.559338 \r\nL 228.337268 207.961594 \r\nL 234.548771 208.132378 \r\nL 240.760274 209.157082 \r\nL 246.971776 206.16836 \r\nL 253.183279 206.16836 \r\nL 259.394782 207.620025 \r\nL 265.606285 206.680712 \r\nL 271.817788 205.656008 \r\nL 278.02929 207.363849 \r\nL 284.240793 206.59532 \r\nL 290.452296 205.399832 \r\nL 296.663799 207.022281 \r\nL 302.875301 207.278456 \r\nL 309.086804 206.851496 \r\nL 315.298307 205.485223 \r\nL 321.50981 204.802086 \r\nL 327.721313 204.716695 \r\nL 333.932815 204.54591 \r\nL 340.144318 204.033559 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p36f9ffa383)\" d=\"M 35.780682 17.083636 \r\nL 41.992185 138.281367 \r\nL 48.203687 136.499805 \r\nL 54.41519 136.843396 \r\nL 60.626693 135.480334 \r\nL 66.838196 135.730088 \r\nL 73.049699 140.790186 \r\nL 79.261201 140.06163 \r\nL 85.472704 140.415307 \r\nL 91.684207 143.854461 \r\nL 97.89571 134.431331 \r\nL 104.107212 147.372815 \r\nL 110.318715 144.413284 \r\nL 116.530218 119.240661 \r\nL 122.741721 147.564288 \r\nL 128.953224 145.632006 \r\nL 135.164726 140.37126 \r\nL 141.376229 150.029794 \r\nL 147.587732 139.460768 \r\nL 153.799235 149.600828 \r\nL 160.010737 137.021437 \r\nL 166.22224 137.379315 \r\nL 172.433743 141.428371 \r\nL 178.645246 144.621244 \r\nL 184.856749 148.938244 \r\nL 191.068251 151.766623 \r\nL 197.279754 151.142843 \r\nL 203.491257 152.264436 \r\nL 209.70276 151.705939 \r\nL 215.914263 148.650713 \r\nL 222.125765 149.489625 \r\nL 228.337268 148.365681 \r\nL 234.548771 153.192524 \r\nL 240.760274 153.395826 \r\nL 246.971776 152.99435 \r\nL 253.183279 151.098206 \r\nL 259.394782 153.495003 \r\nL 265.606285 151.854501 \r\nL 271.817788 150.880114 \r\nL 278.02929 149.901766 \r\nL 284.240793 150.452445 \r\nL 290.452296 156.960556 \r\nL 296.663799 137.551025 \r\nL 302.875301 152.846658 \r\nL 309.086804 152.565394 \r\nL 315.298307 156.948612 \r\nL 321.50981 150.559203 \r\nL 327.721313 147.506527 \r\nL 333.932815 142.168614 \r\nL 340.144318 158.268417 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p36f9ffa383)\" d=\"M 35.780682 214.756364 \r\nL 41.992185 213.780454 \r\nL 48.203687 214.268408 \r\nL 54.41519 214.756364 \r\nL 60.626693 214.756364 \r\nL 66.838196 209.876816 \r\nL 73.049699 209.876816 \r\nL 79.261201 208.900906 \r\nL 85.472704 211.34068 \r\nL 91.684207 208.412952 \r\nL 97.89571 213.780454 \r\nL 104.107212 207.924997 \r\nL 110.318715 207.437043 \r\nL 116.530218 211.34068 \r\nL 122.741721 208.900906 \r\nL 128.953224 206.949088 \r\nL 135.164726 209.876816 \r\nL 141.376229 206.949088 \r\nL 147.587732 212.31659 \r\nL 153.799235 209.876816 \r\nL 160.010737 212.31659 \r\nL 166.22224 212.804544 \r\nL 172.433743 212.804544 \r\nL 178.645246 207.924997 \r\nL 184.856749 208.900906 \r\nL 191.068251 206.949088 \r\nL 197.279754 209.388862 \r\nL 203.491257 207.924997 \r\nL 209.70276 207.437043 \r\nL 215.914263 207.924997 \r\nL 222.125765 207.924997 \r\nL 228.337268 207.924997 \r\nL 234.548771 205.485223 \r\nL 240.760274 204.997269 \r\nL 246.971776 206.949088 \r\nL 253.183279 206.461133 \r\nL 259.394782 204.02136 \r\nL 265.606285 207.437043 \r\nL 271.817788 205.973178 \r\nL 278.02929 206.949088 \r\nL 284.240793 204.509314 \r\nL 290.452296 204.02136 \r\nL 296.663799 204.997269 \r\nL 302.875301 207.437043 \r\nL 309.086804 206.949088 \r\nL 315.298307 205.973178 \r\nL 321.50981 207.924997 \r\nL 327.721313 207.924997 \r\nL 333.932815 204.997269 \r\nL 340.144318 202.557495 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 20.5625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 355.3625 224.64 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 7.2 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 277.221875 74.46875 \r\nL 348.3625 74.46875 \r\nQ 350.3625 74.46875 350.3625 72.46875 \r\nL 350.3625 14.2 \r\nQ 350.3625 12.2 348.3625 12.2 \r\nL 277.221875 12.2 \r\nQ 275.221875 12.2 275.221875 14.2 \r\nL 275.221875 72.46875 \r\nQ 275.221875 74.46875 277.221875 74.46875 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 279.221875 20.298437 \r\nL 299.221875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_12\">\r\n     <!-- loss -->\r\n     <defs>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(307.221875 23.798437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <path d=\"M 279.221875 34.976562 \r\nL 299.221875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\"/>\r\n    <g id=\"text_13\">\r\n     <!-- acc -->\r\n     <defs>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <g transform=\"translate(307.221875 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 279.221875 49.654687 \r\nL 299.221875 49.654687 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\"/>\r\n    <g id=\"text_14\">\r\n     <!-- val_loss -->\r\n     <defs>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n     </defs>\r\n     <g transform=\"translate(307.221875 53.154687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_22\">\r\n     <path d=\"M 279.221875 64.610937 \r\nL 299.221875 64.610937 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_23\"/>\r\n    <g id=\"text_15\">\r\n     <!-- val_acc -->\r\n     <g transform=\"translate(307.221875 68.110937)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p36f9ffa383\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for key in  result.history.keys():\n",
    "    plt.plot(result.history[key], label = key)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "98/98 [==============================] - 0s 4ms/sample - loss: 1.8222 - acc: 0.3061\nloss 1.8222089592291384\nacc 0.30612245\n"
    }
   ],
   "source": [
    "evals = models[\"vgg16_1d\"].evaluate(x_test, y_test)\n",
    "print(\"loss\", evals[0])\n",
    "print(\"acc\",  evals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}